# Experiment 20: MLP with Backpropagation (ReLU Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**ReLU (Rectified Linear Unit) Activation Function:**

*   **Formula:** `ReLU(x) = max(0, x)`
*   **Output Range:** [0, u221e)
*   **Derivative:**
    *   `ReLU_derivative(x) = 1` if `x > 0`
    *   `ReLU_derivative(x) = 0` if `x <= 0`
*   **Advantages:**
    *   Computationally efficient (simple max operation).
    *   Helps mitigate the vanishing gradient problem for positive inputs, often leading to faster training compared to Sigmoid/Tanh.
*   **Disadvantages:**
    *   **Dying ReLU Problem:** Neurons can become inactive if their input consistently results in a negative weighted sum, causing their output (and gradient) to be zero. This prevents weight updates for that neuron.
*   **Use:** Very common activation function for hidden layers in deep neural networks.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **ReLU** activation.
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.

**Backpropagation with ReLU:**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the ReLU derivative:

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3).
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = ReLU(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = ReLU(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE or Binary Cross-Entropy)
    *   MSE: `Loss = 1/m * sum((Y - Y_hat)^2)`
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)` (Derivative w.r.t Z3)
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (ReLU):**
        *   `dZ2 = dA2 * ReLU_derivative(Z2)` (Note: Derivative depends on Z2, not A2)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (ReLU):**
        *   `dZ1 = dA1 * ReLU_derivative(Z1)` (Note: Derivative depends on Z1, not A1)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function relu(x):
  Return max(0, x)

Function relu_derivative(x): // x is the input Z to ReLU
  If x > 0:
    Return 1
  Else:
    Return 0

// Initialize Weights and Biases (same as Exp 19)
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // ... (same initialization)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using ReLU for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = relu(Z1)
    Z2 = A1.dot(W2) + b2
    A2 = relu(Z2)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples // Derivative of MSE
    dYhat_dZ3 = sigmoid_derivative(Y_hat) // Derivative of sigmoid(Z3) w.r.t Z3
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (ReLU)
    // Apply element-wise: dA2 * derivative(Z2)
    dZ2 = dA2 * relu_derivative(Z2) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (ReLU)
    // Apply element-wise: dA1 * derivative(Z1)
    dZ1 = dA1 * relu_derivative(Z1) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to Exp 19, define N, data, hyperparameters)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (for output layer)
def sigmoid(x):
    # Add clipping to prevent overflow/underflow in exp
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# ReLU activation function and its derivative
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x): # x is the input Z to ReLU
    return (x > 0).astype(float) # Returns 1 if x > 0, else 0

class MLP_ReLU_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs

        # Initialize weights and biases (He initialization often preferred for ReLU, but using simple randn for consistency)
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * np.sqrt(2. / self.n_inputs) # He init scale
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * np.sqrt(2. / self.n_hidden1) # He init scale
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Output layer (Sigmoid) might use smaller init
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders for activations and weighted sums during backprop
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = relu(self.Z1)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = relu(self.Z2)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0] # Number of samples

        # Calculate error and loss derivative (using MSE for simplicity)
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (ReLU)
        dZ2 = dA2 * relu_derivative(self.Z2) # Use Z2 for ReLU derivative
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (ReLU)
        dZ1 = dA1 * relu_derivative(self.Z1) # Use Z1 for ReLU derivative
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            # Forward pass
            Y_hat = self.forward_pass(X)

            # Calculate loss (MSE)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)

            # Backward pass and update weights
            self.backward_pass(X, Y, learning_rate)

            # Print progress
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}") # Optionally print weights
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    # Example: XOR-like problem for N inputs
    N = 4 # Number of inputs
    num_samples = 100

    # Generate random binary input data
    X_train = np.random.randint(0, 2, size=(num_samples, N))

    # Generate synthetic binary output Y (e.g., Output is 1 if sum of inputs is odd)
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    # Define network structure
    n_hidden1 = 16 # ReLU networks might benefit from more neurons
    n_hidden2 = 8
    n_outputs = 1 # Single output neuron (Sigmoid)

    # Hyperparameters (May need tuning for ReLU)
    epochs = 3000 # Might converge faster or need different number
    learning_rate = 0.05 # Often requires smaller learning rate than Sigmoid

    # Create and train the MLP
    mlp = MLP_ReLU_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs)
    print("\n--- Training MLP with ReLU Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")

    # Display final parameters
    mlp.display_parameters()

    # Make predictions
    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)

    # Calculate accuracy
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title('Training Loss over Epochs (ReLU)')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`relu` and `relu_derivative`:** Implement the ReLU function and its derivative. Note that the derivative takes the *input* to ReLU (`Z`) as its argument.
2.  **`sigmoid` and `sigmoid_derivative`:** Kept for the output layer.
3.  **`MLP_ReLU_SigmoidOutput` Class:**
    *   `__init__`: Initializes the network. He initialization (`np.sqrt(2. / n_inputs)`) is used for weights connected to ReLU layers, which is often recommended.
    *   `forward_pass`: Uses `relu` for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `relu_derivative(self.Z1)` and `relu_derivative(self.Z2)` when calculating `dZ1` and `dZ2` respectively. The output layer gradient calculation remains the same as it depends on the Sigmoid activation.
    *   `train`, `predict`, `display_parameters`: Similar functionality to the Sigmoid version.
4.  **Main Block:**
    *   Sets up the data and network parameters.
    *   Note that hyperparameters like hidden layer sizes, `epochs`, and `learning_rate` might need different values compared to the Sigmoid network for optimal performance. ReLU networks can sometimes train faster but might require smaller learning rates to avoid divergence.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using ReLU in the hidden layers and Sigmoid in the output layer, trained with backpropagation.