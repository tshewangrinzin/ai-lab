# Experiment 23: MLP with Backpropagation (ELU Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**ELU (Exponential Linear Unit) Activation Function:**

*   **Formula:**
    *   `ELU(x) = x` if `x > 0`
    *   `ELU(x) = alpha * (exp(x) - 1)` if `x <= 0`
    *   `alpha` is a positive hyperparameter, often set to 1.0.
*   **Output Range:** (`alpha * (exp(-inf) - 1)`) = `-alpha`, u221e)
*   **Derivative:**
    *   `ELU_derivative(x) = 1` if `x > 0`
    *   `ELU_derivative(x) = ELU(x) + alpha` if `x <= 0` (Note: derivative depends on the ELU output for x<=0)
*   **Advantages:**
    *   Like Leaky ReLU, it addresses the dying ReLU problem by having non-zero gradients for negative inputs.
    *   Produces negative outputs, which can help push the mean activation closer to zero (similar to Tanh), potentially improving learning.
    *   Often reported to lead to faster convergence and better generalization than ReLU/Leaky ReLU in some tasks.
*   **Disadvantages:**
    *   Computationally more expensive than ReLU/Leaky ReLU due to the exponential function.
    *   Introduces the `alpha` hyperparameter.
*   **Use:** An alternative to ReLU and its variants, especially when seeking faster convergence or potentially better performance.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **ELU** activation (with a chosen `alpha`).
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.

**Backpropagation with ELU:**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the ELU derivative:

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3).
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = ELU(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = ELU(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE)
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (ELU):**
        *   `dZ2 = dA2 * ELU_derivative(Z2, A2, alpha)` (Note: Derivative depends on Z2 and A2 for x<=0)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (ELU):**
        *   `dZ1 = dA1 * ELU_derivative(Z1, A1, alpha)` (Note: Derivative depends on Z1 and A1 for x<=0)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs
Alpha = 1.0 // ELU parameter

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function elu(x, alpha):
  If x > 0:
    Return x
  Else:
    Return alpha * (exp(x) - 1)

Function elu_derivative(z, a, alpha): // z is input, a is elu(z)
  If z > 0:
    Return 1
  Else:
    Return a + alpha // Derivative for x <= 0 is ELU(x) + alpha

// Initialize Weights and Biases (He init often suitable)
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // ... (He initialization recommended)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate, alpha):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using ELU for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = elu(Z1, alpha)
    Z2 = A1.dot(W2) + b2
    A2 = elu(Z2, alpha)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples
    dYhat_dZ3 = sigmoid_derivative(Y_hat)
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (ELU)
    // Apply element-wise: dA2 * derivative(Z2, A2, alpha)
    dZ2 = dA2 * elu_derivative(Z2, A2, alpha) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (ELU)
    // Apply element-wise: dA1 * derivative(Z1, A1, alpha)
    dZ1 = dA1 * elu_derivative(Z1, A1, alpha) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to previous experiments, define N, data, hyperparameters, alpha)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate, alpha)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (for output layer)
def sigmoid(x):
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# ELU activation function and its derivative
def elu(x, alpha=1.0):
    x_clipped = np.clip(x, -500, 500) # Clip input to exp
    return np.where(x > 0, x, alpha * (np.exp(x_clipped) - 1))

def elu_derivative(z, a, alpha=1.0): # z is input, a is elu(z)
    # Note: The derivative for z <= 0 is a + alpha
    return np.where(z > 0, 1, a + alpha)

class MLP_ELU_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs, alpha=1.0):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs
        self.alpha = alpha # Store alpha

        # Initialize weights and biases (He initialization often suitable)
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * np.sqrt(2. / self.n_inputs)
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * np.sqrt(2. / self.n_hidden1)
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Output layer (Sigmoid)
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = elu(self.Z1, self.alpha)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = elu(self.Z2, self.alpha)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0]
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (ELU)
        # Pass Z2 and A2 to the derivative function
        dZ2 = dA2 * elu_derivative(self.Z2, self.A2, self.alpha)
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (ELU)
        # Pass Z1 and A1 to the derivative function
        dZ1 = dA1 * elu_derivative(self.Z1, self.A1, self.alpha)
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            Y_hat = self.forward_pass(X)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)
            self.backward_pass(X, Y, learning_rate)
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}")
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    N = 4
    num_samples = 100
    X_train = np.random.randint(0, 2, size=(num_samples, N))
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    n_hidden1 = 16
    n_hidden2 = 8
    n_outputs = 1
    elu_alpha = 1.0 # Define alpha for ELU (common default)

    epochs = 3000
    learning_rate = 0.05 # May need tuning

    mlp = MLP_ELU_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs, alpha=elu_alpha)
    print(f"\n--- Training MLP with ELU (alpha={elu_alpha}) Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")
    mlp.display_parameters()

    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title(f'Training Loss over Epochs (ELU alpha={elu_alpha})')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`elu` and `elu_derivative`:** Implement the ELU function and its derivative. Note that the derivative for `x <= 0` is `ELU(x) + alpha`, so it depends on both the input `z` (to check the condition) and the output `a = elu(z)`.
2.  **`sigmoid` and `sigmoid_derivative`:** Kept for the output layer.
3.  **`MLP_ELU_SigmoidOutput` Class:**
    *   `__init__`: Initializes the network, storing the `alpha` value (defaulting to 1.0). He initialization is often suitable.
    *   `forward_pass`: Uses `elu` (passing `self.alpha`) for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `elu_derivative(self.Z1, self.A1, self.alpha)` and `elu_derivative(self.Z2, self.A2, self.alpha)` when calculating `dZ1` and `dZ2` respectively, passing both the pre-activation (`Z`) and activation (`A`) values.
    *   `train`, `predict`, `display_parameters`: Similar functionality.
4.  **Main Block:**
    *   Sets up data, network parameters, and defines the `elu_alpha`.
    *   Instantiates the `MLP_ELU_SigmoidOutput` class, passing `alpha`.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using ELU in the hidden layers and Sigmoid in the output layer, trained with backpropagation, showcasing the implementation details of ELU and its derivative.