# Experiment 22: MLP with Backpropagation (Leaky ReLU Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**Leaky ReLU (Leaky Rectified Linear Unit) Activation Function:**

*   **Formula:** `LeakyReLU(x) = max(alpha * x, x)` where `alpha` is a small positive constant (e.g., 0.01).
    *   If `x > 0`, `LeakyReLU(x) = x`
    *   If `x <= 0`, `LeakyReLU(x) = alpha * x`
*   **Output Range:** (-u221e, u221e)
*   **Derivative:**
    *   `LeakyReLU_derivative(x) = 1` if `x > 0`
    *   `LeakyReLU_derivative(x) = alpha` if `x <= 0`
*   **Advantages:**
    *   Addresses the "Dying ReLU" problem by allowing a small, non-zero gradient when the unit is not active (`x <= 0`).
    *   Retains the computational efficiency of ReLU for positive inputs.
*   **Disadvantages:**
    *   Results are not always consistently better than standard ReLU.
    *   Introduces an extra hyperparameter (`alpha`) to tune.
*   **Use:** An alternative to ReLU, particularly when encountering the dying ReLU issue.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **Leaky ReLU** activation (with a chosen `alpha`).
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.

**Backpropagation with Leaky ReLU:**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the Leaky ReLU derivative:

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3).
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = LeakyReLU(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = LeakyReLU(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE)
    *   MSE: `Loss = 1/m * sum((Y - Y_hat)^2)`
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (Leaky ReLU):**
        *   `dZ2 = dA2 * LeakyReLU_derivative(Z2)` (Note: Derivative depends on Z2)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (Leaky ReLU):**
        *   `dZ1 = dA1 * LeakyReLU_derivative(Z1)` (Note: Derivative depends on Z1)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs
Alpha = 0.01 // Leaky ReLU parameter

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function leaky_relu(x, alpha):
  Return max(alpha * x, x)

Function leaky_relu_derivative(x, alpha): // x is the input Z to Leaky ReLU
  If x > 0:
    Return 1
  Else:
    Return alpha

// Initialize Weights and Biases (same as Exp 20 - He init often suitable)
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // ... (He initialization recommended)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate, alpha):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using Leaky ReLU for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = leaky_relu(Z1, alpha)
    Z2 = A1.dot(W2) + b2
    A2 = leaky_relu(Z2, alpha)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples // Derivative of MSE
    dYhat_dZ3 = sigmoid_derivative(Y_hat) // Derivative of sigmoid(Z3) w.r.t Z3
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (Leaky ReLU)
    // Apply element-wise: dA2 * derivative(Z2, alpha)
    dZ2 = dA2 * leaky_relu_derivative(Z2, alpha) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (Leaky ReLU)
    // Apply element-wise: dA1 * derivative(Z1, alpha)
    dZ1 = dA1 * leaky_relu_derivative(Z1, alpha) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to Exp 19/20/21, define N, data, hyperparameters, alpha)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate, alpha)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (for output layer)
def sigmoid(x):
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# Leaky ReLU activation function and its derivative
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, x * alpha)

def leaky_relu_derivative(x, alpha=0.01): # x is the input Z to Leaky ReLU
    return np.where(x > 0, 1, alpha)

class MLP_LeakyReLU_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs, alpha=0.01):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs
        self.alpha = alpha # Store alpha

        # Initialize weights and biases (He initialization often suitable for ReLU variants)
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * np.sqrt(2. / self.n_inputs)
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * np.sqrt(2. / self.n_hidden1)
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Output layer (Sigmoid)
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = leaky_relu(self.Z1, self.alpha)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = leaky_relu(self.Z2, self.alpha)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0]
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (Leaky ReLU)
        dZ2 = dA2 * leaky_relu_derivative(self.Z2, self.alpha) # Use Z2 and alpha
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (Leaky ReLU)
        dZ1 = dA1 * leaky_relu_derivative(self.Z1, self.alpha) # Use Z1 and alpha
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            Y_hat = self.forward_pass(X)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)
            self.backward_pass(X, Y, learning_rate)
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}")
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    N = 4
    num_samples = 100
    X_train = np.random.randint(0, 2, size=(num_samples, N))
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    n_hidden1 = 16
    n_hidden2 = 8
    n_outputs = 1
    leaky_alpha = 0.01 # Define alpha for Leaky ReLU

    epochs = 3000
    learning_rate = 0.05 # Similar LR as ReLU might work

    mlp = MLP_LeakyReLU_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs, alpha=leaky_alpha)
    print(f"\n--- Training MLP with Leaky ReLU (alpha={leaky_alpha}) Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")
    mlp.display_parameters()

    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title(f'Training Loss over Epochs (Leaky ReLU alpha={leaky_alpha})')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`leaky_relu` and `leaky_relu_derivative`:** Implement the Leaky ReLU function and its derivative. Both take `alpha` as an argument (defaulting to 0.01). The derivative depends on the *input* `Z`.
2.  **`sigmoid` and `sigmoid_derivative`:** Kept for the output layer.
3.  **`MLP_LeakyReLU_SigmoidOutput` Class:**
    *   `__init__`: Initializes the network, storing the `alpha` value. He initialization is used, similar to standard ReLU.
    *   `forward_pass`: Uses `leaky_relu` (passing `self.alpha`) for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `leaky_relu_derivative(self.Z1, self.alpha)` and `leaky_relu_derivative(self.Z2, self.alpha)` when calculating `dZ1` and `dZ2` respectively.
    *   `train`, `predict`, `display_parameters`: Similar functionality.
4.  **Main Block:**
    *   Sets up data, network parameters, and defines the `leaky_alpha`.
    *   Instantiates the `MLP_LeakyReLU_SigmoidOutput` class, passing `alpha`.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using Leaky ReLU in the hidden layers and Sigmoid in the output layer, trained with backpropagation, demonstrating how to incorporate this ReLU variant.