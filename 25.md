# Experiment 25: MLP with Backpropagation (GELU Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**GELU (Gaussian Error Linear Unit) Activation Function:**

*   **Formula:** `GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))`
    *   An approximation often used is: `GELU_approx(x) = x * sigmoid(1.702 * x)`
*   **Output Range:** Approximately (-0.17, u221e).
*   **Derivative (using approximation):** Let `s = sigmoid(1.702 * x)`. Then `GELU_approx'(x) = s + x * (1.702 * s * (1 - s))`
*   **Advantages:**
    *   Combines properties of ReLU (linearity for positive x), dropout (stochastic regularization implicitly), and zoneout.
    *   Smooth and non-monotonic.
    *   State-of-the-art results in NLP (e.g., BERT, GPT) and increasingly used in computer vision.
    *   Avoids the dying ReLU problem.
*   **Disadvantages:**
    *   Computationally more expensive than ReLU, Leaky ReLU, ELU, and even Swish (especially the exact form).
*   **Use:** Increasingly popular, especially in Transformer-based models and other large networks.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **GELU** activation (using the approximation for simplicity).
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.

**Backpropagation with GELU (Approximation):**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the GELU approximation's derivative:

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3).
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = GELU_approx(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = GELU_approx(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE)
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (GELU Approx):**
        *   `dZ2 = dA2 * GELU_approx_derivative(Z2)` (Note: Derivative depends on Z2)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (GELU Approx):**
        *   `dZ1 = dA1 * GELU_approx_derivative(Z1)` (Note: Derivative depends on Z1)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function gelu_approx(x):
  Return x * sigmoid(1.702 * x)

Function gelu_approx_derivative(x): // x is the input Z
  s = sigmoid(1.702 * x)
  Return s + x * 1.702 * s * (1 - s)

// Initialize Weights and Biases (He init often suitable)
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // ... (He initialization recommended)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using GELU Approx for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = gelu_approx(Z1)
    Z2 = A1.dot(W2) + b2
    A2 = gelu_approx(Z2)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples
    dYhat_dZ3 = sigmoid_derivative(Y_hat)
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (GELU Approx)
    // Apply element-wise: dA2 * derivative(Z2)
    dZ2 = dA2 * gelu_approx_derivative(Z2) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (GELU Approx)
    // Apply element-wise: dA1 * derivative(Z1)
    dZ1 = dA1 * gelu_approx_derivative(Z1) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to previous experiments, define N, data, hyperparameters)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (used by GELU approx and output layer)
def sigmoid(x):
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# GELU approximation activation function and its derivative
def gelu_approx(x):
    return x * sigmoid(1.702 * x)

def gelu_approx_derivative(x): # x is the input Z
    s = sigmoid(1.702 * x)
    return s + x * 1.702 * s * (1 - s)

# --- Optional: Exact GELU and its derivative (more complex) ---
# from scipy.stats import norm
# def gelu_exact(x):
#     return x * norm.cdf(x)
# 
# def gelu_exact_derivative(x):
#     return norm.cdf(x) + x * norm.pdf(x)
# -------------------------------------------------------------

class MLP_GELU_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs

        # Initialize weights and biases (He initialization often suitable)
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * np.sqrt(2. / self.n_inputs)
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * np.sqrt(2. / self.n_hidden1)
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Output layer (Sigmoid)
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = gelu_approx(self.Z1) # Using approximation
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = gelu_approx(self.Z2) # Using approximation
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0]
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (GELU Approx)
        # Pass Z2 to the derivative function
        dZ2 = dA2 * gelu_approx_derivative(self.Z2)
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (GELU Approx)
        # Pass Z1 to the derivative function
        dZ1 = dA1 * gelu_approx_derivative(self.Z1)
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            Y_hat = self.forward_pass(X)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)
            self.backward_pass(X, Y, learning_rate)
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}")
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    N = 4
    num_samples = 100
    X_train = np.random.randint(0, 2, size=(num_samples, N))
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    n_hidden1 = 16
    n_hidden2 = 8
    n_outputs = 1

    epochs = 3000
    learning_rate = 0.05 # May need tuning

    mlp = MLP_GELU_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs)
    print(f"\n--- Training MLP with GELU (Approximation) Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")
    mlp.display_parameters()

    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title('Training Loss over Epochs (GELU Approx)')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`sigmoid` and `sigmoid_derivative`:** Kept as the GELU approximation uses sigmoid internally, and it's also used for the output layer.
2.  **`gelu_approx` and `gelu_approx_derivative`:** Implement the common approximation `x * sigmoid(1.702 * x)` and its derivative. The derivative depends only on the input `x` (or `Z` in the network context).
3.  **`MLP_GELU_SigmoidOutput` Class:**
    *   `__init__`: Standard initialization. He initialization is often suitable.
    *   `forward_pass`: Uses `gelu_approx` for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `gelu_approx_derivative(self.Z1)` and `gelu_approx_derivative(self.Z2)` when calculating `dZ1` and `dZ2` respectively, passing only the pre-activation (`Z`) values.
    *   `train`, `predict`, `display_parameters`: Similar functionality.
4.  **Main Block:**
    *   Sets up data and network parameters.
    *   Instantiates the `MLP_GELU_SigmoidOutput` class.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using the GELU approximation in the hidden layers and Sigmoid in the output layer, trained with backpropagation. It demonstrates how to integrate this modern activation function, often found in state-of-the-art models.