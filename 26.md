# Experiment 26: MLP with Backpropagation (SELU Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**SELU (Scaled Exponential Linear Unit) Activation Function:**

*   **Formula:** `SELU(x) = scale * ELU(x, alpha)`
    *   Where `ELU(x, alpha)` is the standard ELU function.
    *   `scale` (lambda) u2248 1.0507
    *   `alpha` u2248 1.6733
    *   Explicitly: 
        *   `SELU(x) = scale * x` if `x > 0`
        *   `SELU(x) = scale * alpha * (exp(x) - 1)` if `x <= 0`
*   **Output Range:** (`scale * alpha * (exp(-inf) - 1)`) u2248 -1.758, u221e)
*   **Derivative:**
    *   `SELU_derivative(x) = scale` if `x > 0`
    *   `SELU_derivative(x) = scale * alpha * exp(x)` if `x <= 0` 
       (Note: This can also be written as `SELU(x) + scale * alpha` for `x <= 0`)
*   **Advantages:**
    *   **Self-Normalizing:** With specific weight initialization (Lecun normal) and network conditions, SELU can push neuron activations towards zero mean and unit variance, helping to prevent vanishing/exploding gradients without explicit batch normalization.
    *   Addresses the dying ReLU problem.
*   **Disadvantages:**
    *   The self-normalizing property requires specific weight initialization (`lecun_normal`) and assumes a certain network architecture (sequential dense layers).
    *   Computationally more expensive than ReLU due to the exponential function.
    *   The fixed `alpha` and `scale` values are derived theoretically and might not be optimal for all situations.
*   **Use:** Designed for deep feedforward networks where self-normalization is desired. Less common than ReLU/variants but powerful when its conditions are met.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **SELU** activation.
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.
*   **Crucially:** Use Lecun Normal weight initialization for layers preceding SELU units.

**Backpropagation with SELU:**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the SELU derivative:

1.  **Initialization:** Initialize weights (W1, W2) using Lecun Normal, W3 can use standard init. Initialize biases (b1, b2, b3) to zero.
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = SELU(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = SELU(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE)
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (SELU):**
        *   `dZ2 = dA2 * SELU_derivative(Z2)` (Note: Derivative depends on Z2)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (SELU):**
        *   `dZ1 = dA1 * SELU_derivative(Z1)` (Note: Derivative depends on Z1)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs

// SELU Constants
Scale = 1.0507
Alpha = 1.6733

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function selu(x, scale, alpha):
  If x > 0:
    Return scale * x
  Else:
    Return scale * alpha * (exp(x) - 1)

Function selu_derivative(x, scale, alpha): // x is the input Z
  If x > 0:
    Return scale
  Else:
    Return scale * alpha * exp(x)

// Initialize Weights and Biases (Lecun Normal for SELU layers)
Function initialize_mlp_selu(N, h1_size, h2_size, output_size):
  // Lecun Normal: stddev = sqrt(1 / n_inputs)
  W1 = random_normal(mean=0, stddev=sqrt(1/N), size=(N, h1_size))
  b1 = zeros((1, h1_size))
  W2 = random_normal(mean=0, stddev=sqrt(1/h1_size), size=(h1_size, h2_size))
  b2 = zeros((1, h2_size))
  // Output layer (Sigmoid) - standard init might be okay
  W3 = random_normal(mean=0, stddev=0.1, size=(h2_size, output_size))
  b3 = zeros((1, output_size))
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate, scale, alpha):
  W1, b1, W2, b2, W3, b3 = initialize_mlp_selu(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using SELU for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = selu(Z1, scale, alpha)
    Z2 = A1.dot(W2) + b2
    A2 = selu(Z2, scale, alpha)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples
    dYhat_dZ3 = sigmoid_derivative(Y_hat)
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (SELU)
    // Apply element-wise: dA2 * derivative(Z2, scale, alpha)
    dZ2 = dA2 * selu_derivative(Z2, scale, alpha) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (SELU)
    // Apply element-wise: dA1 * derivative(Z1, scale, alpha)
    dZ1 = dA1 * selu_derivative(Z1, scale, alpha) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to previous experiments, define N, data, hyperparameters)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate, Scale, Alpha)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (for output layer)
def sigmoid(x):
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# SELU constants
SELU_SCALE = 1.0507009873554804934193349852946
SELU_ALPHA = 1.6732632423543772848170429916717

# SELU activation function and its derivative
def selu(x, scale=SELU_SCALE, alpha=SELU_ALPHA):
    x_clipped = np.clip(x, -500, 500) # Clip input to exp
    return scale * np.where(x > 0, x, alpha * (np.exp(x_clipped) - 1))

def selu_derivative(x, scale=SELU_SCALE, alpha=SELU_ALPHA): # x is the input Z
    x_clipped = np.clip(x, -500, 500) # Clip input to exp
    return scale * np.where(x > 0, 1, alpha * np.exp(x_clipped))

class MLP_SELU_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs

        # Initialize weights with Lecun Normal and biases to zero for SELU layers
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * np.sqrt(1. / self.n_inputs)
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * np.sqrt(1. / self.n_hidden1)
        self.b2 = np.zeros((1, self.n_hidden2))
        
        # Output layer (Sigmoid) - standard init might be okay, or Lecun Normal
        # self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * np.sqrt(1. / self.n_hidden2)
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Using small random init
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = selu(self.Z1)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = selu(self.Z2)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0]
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (SELU)
        # Pass Z2 to the derivative function
        dZ2 = dA2 * selu_derivative(self.Z2)
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (SELU)
        # Pass Z1 to the derivative function
        dZ1 = dA1 * selu_derivative(self.Z1)
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            Y_hat = self.forward_pass(X)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)
            self.backward_pass(X, Y, learning_rate)
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}")
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    N = 4
    num_samples = 100
    X_train = np.random.randint(0, 2, size=(num_samples, N))
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    n_hidden1 = 16
    n_hidden2 = 8
    n_outputs = 1

    epochs = 3000
    learning_rate = 0.05 # May need tuning

    mlp = MLP_SELU_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs)
    print(f"\n--- Training MLP with SELU Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")
    mlp.display_parameters()

    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title('Training Loss over Epochs (SELU)')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`sigmoid` and `sigmoid_derivative`:** Kept for the output layer.
2.  **`SELU_SCALE`, `SELU_ALPHA`:** Constants defined for SELU.
3.  **`selu` and `selu_derivative`:** Implement the SELU function and its derivative using the defined constants. The derivative depends only on the input `x` (or `Z`).
4.  **`MLP_SELU_SigmoidOutput` Class:**
    *   `__init__`: **Crucially**, initializes weights `W1` and `W2` using **Lecun Normal initialization** (`stddev = sqrt(1 / n_inputs)`). Biases are initialized to zero. The output layer weight `W3` can use a different initialization as it's followed by Sigmoid.
    *   `forward_pass`: Uses `selu` for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `selu_derivative(self.Z1)` and `selu_derivative(self.Z2)` when calculating `dZ1` and `dZ2` respectively, passing only the pre-activation (`Z`) values.
    *   `train`, `predict`, `display_parameters`: Similar functionality.
5.  **Main Block:**
    *   Sets up data and network parameters.
    *   Instantiates the `MLP_SELU_SigmoidOutput` class.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using SELU in the hidden layers and Sigmoid in the output layer, trained with backpropagation. It highlights the specific weight initialization (Lecun Normal) required for SELU to potentially achieve its self-normalizing properties.