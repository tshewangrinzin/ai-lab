# Experiment 18: Simple Multi-Layer Perceptron (4 Inputs, 1 Hidden, 2 Outputs)

## Theory

**Multi-Layer Perceptron (MLP):**

An MLP is a fundamental type of feedforward artificial neural network (ANN). It consists of an input layer, one or more hidden layers, and an output layer. Neurons in the hidden and output layers typically use non-linear activation functions to learn complex data patterns.

*   **Input Layer:** Receives the raw input features. In this case, 4 binary inputs (0 or 1).
*   **Hidden Layer:** An intermediate layer that transforms the input data. This MLP has one hidden layer.
*   **Output Layer:** Produces the final prediction or classification. Here, it has two neurons, each producing a binary output (or a value interpretable as binary, like probabilities from Sigmoid).
*   **Neurons:** Process weighted inputs plus a bias, apply an activation function, and pass the output forward.
*   **Weights & Biases:** Learnable parameters that define the network's behavior. Initialized randomly.
*   **Activation Function:** Introduces non-linearity (e.g., Sigmoid, ReLU, Tanh).
*   **Feedforward:** Data flows strictly from input to output.

**Task Specifics:**

This experiment requires defining an MLP with:

*   4 binary inputs.
*   One hidden layer (size can be chosen, e.g., `h_size`).
*   An output layer with 2 neurons for two binary outputs.

Similar to Experiment 17, the focus is on initializing the weights and biases randomly and displaying them. No training (backpropagation) is involved.

**Weight Matrix Dimensions:**

*   Input to Hidden Layer (W1): `(4, h_size)`
*   Bias for Hidden Layer (b1): `(1, h_size)`
*   Hidden Layer to Output Layer (W2): `(h_size, 2)`
*   Bias for Output Layer (b2): `(1, 2)`

## Pseudocode/Algorithm

```
// Define Network Structure
Input_size = 4
Define h_size (number of neurons in the hidden layer)
Output_size = 2

// Initialize Weights and Biases Randomly
Function initialize_mlp(input_size, h_size, output_size):
  // Weights
  W1 = Generate random matrix of size (input_size, h_size)
  W2 = Generate random matrix of size (h_size, output_size)

  // Biases
  b1 = Generate random vector of size (1, h_size)
  b2 = Generate random vector of size (1, output_size)

  Return W1, b1, W2, b2

// Display Function
Function display_parameters(W1, b1, W2, b2):
  Print "--- MLP Structure ---"
  Print "Input Layer Size:", 4
  Print "Hidden Layer Size:", h_size
  Print "Output Layer Size:", 2

  Print "\n--- Initial Random Weights ---"
  Print "W1 (Input -> Hidden) Matrix Shape:", shape(W1)
  Print W1
  Print "W2 (Hidden -> Output) Matrix Shape:", shape(W2)
  Print W2

  Print "\n--- Initial Random Biases ---"
  Print "b1 (Hidden) Vector Shape:", shape(b1)
  Print b1
  Print "b2 (Output) Vector Shape:", shape(b2)
  Print b2

  // Interpret "number of steps" as number of layers
  Print "\nNumber of Layers (Steps):", 3 // Input, Hidden, Output

Main:
  input_size = 4
  Set h_size (e.g., 5)
  output_size = 2

  (W1, b1, W2, b2) = initialize_mlp(input_size, h_size, output_size)

  display_parameters(W1, b1, W2, b2)
```

## Python Code

```python
import numpy as np

def initialize_mlp(n_inputs, n_hidden, n_outputs):
    """Initializes weights and biases for a 1-hidden layer MLP randomly.

    Args:
        n_inputs (int): Number of input features (4 for this case).
        n_hidden (int): Number of neurons in the hidden layer.
        n_outputs (int): Number of output neurons (2 for this case).

    Returns:
        tuple: Contains weight matrices (W1, W2) and bias vectors (b1, b2).
    """
    # Initialize weights with small random values
    W1 = np.random.randn(n_inputs, n_hidden)
    W2 = np.random.randn(n_hidden, n_outputs)

    # Initialize biases
    b1 = np.random.randn(1, n_hidden)
    b2 = np.random.randn(1, n_outputs)

    return W1, b1, W2, b2

def display_parameters(n_inputs, n_hidden, n_outputs, W1, b1, W2, b2):
    """Displays the network structure and initial parameters."""
    print("--- MLP Structure ---")
    print(f"Input Layer Size: {n_inputs}")
    print(f"Hidden Layer Size: {n_hidden}")
    print(f"Output Layer Size: {n_outputs}")

    print("\n--- Initial Random Weights ---")
    print(f"W1 (Input -> Hidden) Matrix Shape: {W1.shape}")
    print(W1)
    print(f"\nW2 (Hidden -> Output) Matrix Shape: {W2.shape}")
    print(W2)

    print("\n--- Initial Random Biases ---")
    print(f"b1 (Hidden) Vector Shape: {b1.shape}")
    print(b1)
    print(f"\nb2 (Output) Vector Shape: {b2.shape}")
    print(b2)

    # Interpret "number of steps" as the number of layers
    print(f"\nNumber of Layers (including input): 3")
    print(f"Number of Computational Layers (Steps): 2")

# --- Main Execution ---
if __name__ == "__main__":
    n_inputs = 4 # Fixed number of binary inputs

    # Define the size of the hidden layer (can be adjusted)
    n_hidden = 5 # Example size
    n_outputs = 2 # Two binary outputs

    print(f"Initializing MLP with Inputs={n_inputs}, Hidden={n_hidden}, Outputs={n_outputs}")

    # Initialize the MLP parameters
    W1, b1, W2, b2 = initialize_mlp(n_inputs, n_hidden, n_outputs)

    # Display the structure and initial parameters
    display_parameters(n_inputs, n_hidden, n_outputs, W1, b1, W2, b2)

```

**Explanation:**

1.  **`initialize_mlp` Function:** Creates weight matrices `W1` (Input to Hidden) and `W2` (Hidden to Output), and bias vectors `b1` (Hidden) and `b2` (Output) using `numpy.random.randn` for random initialization.
2.  **`display_parameters` Function:** Prints the network architecture (layer sizes) and the shapes and values of the initialized weights and biases.
3.  **Main Block:**
    *   Sets `n_inputs` to 4 and `n_outputs` to 2 as specified.
    *   Sets a size for the hidden layer (`n_hidden`).
    *   Calls `initialize_mlp` to get the random parameters.
    *   Calls `display_parameters` to show the results.

This code sets up the required MLP structure with 4 inputs, 1 hidden layer, and 2 outputs, initializing its parameters randomly and displaying them.