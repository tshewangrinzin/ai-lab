# Experiment 19: MLP with Backpropagation (Sigmoid Activation)

## Theory

**Multi-Layer Perceptron (MLP):**

An MLP is a feedforward neural network with one or more hidden layers. It learns complex, non-linear relationships between inputs and outputs.

**Backpropagation Algorithm:**

Backpropagation is the standard algorithm for training MLPs. It's a supervised learning algorithm that adjusts the network's weights and biases to minimize the difference (error) between the network's output and the desired target output.

**Steps:**

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3) randomly (often small values).
2.  **Forward Pass:**
    *   Present an input vector `X` to the network.
    *   Calculate the weighted sum and apply the activation function for each neuron layer by layer:
        *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = sigmoid(Z1)`
        *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = sigmoid(Z2)`
        *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** Compute the difference between the predicted output `Y_hat` and the target output `Y`. A common loss function for binary classification is Mean Squared Error (MSE) or Binary Cross-Entropy.
    *   MSE: `Loss = 1/m * sum((Y - Y_hat)^2)` (where m is the number of samples)
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate the gradient of the loss function with respect to the output layer's activation (`dLoss/dY_hat`).
    *   Propagate this error backward through the network, layer by layer, calculating the gradients of the loss with respect to weights and biases.
    *   **Output Layer Gradients:**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Z3)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
    *   **Hidden Layer 2 Gradients:**
        *   `dA2 = dZ3.dot(W3.T)`
        *   `dZ2 = dA2 * sigmoid_derivative(Z2)`
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
    *   **Hidden Layer 1 Gradients:**
        *   `dA1 = dZ2.dot(W2.T)`
        *   `dZ1 = dA1 * sigmoid_derivative(Z1)`
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:** Adjust the weights and biases using gradient descent (or a variant like Adam, RMSprop).
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Repeat steps 2-5 for a fixed number of epochs (iterations over the entire dataset) or until the error converges to a satisfactory level.

**Sigmoid Activation Function:**

*   Formula: `sigmoid(x) = 1 / (1 + exp(-x))`
*   Output Range: (0, 1)
*   Derivative: `sigmoid_derivative(x) = sigmoid(x) * (1 - sigmoid(x))`
*   Use: Often used in hidden layers (historically) and especially in the output layer for binary classification problems, as its output can be interpreted as a probability.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers.
*   One output neuron.
*   Use Sigmoid activation in all layers (hidden and output).
*   Implement the backpropagation algorithm for training.

## Pseudocode/Algorithm

```
// Define Network Structure
Input: N (number of inputs)
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs

// Activation Function
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

// Initialize Weights and Biases
Function initialize_mlp(N, h1_size, h2_size, output_size):
  W1 = Random matrix (N, h1_size)
  b1 = Random vector (1, h1_size)
  W2 = Random matrix (h1_size, h2_size)
  b2 = Random vector (1, h2_size)
  W3 = Random matrix (h2_size, output_size)
  b3 = Random vector (1, output_size)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass ---
    Z1 = X_train.dot(W1) + b1
    A1 = sigmoid(Z1)
    Z2 = A1.dot(W2) + b2
    A2 = sigmoid(Z2)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3)

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients
    dLoss_dYhat = -2 * error / number_of_samples // Derivative of MSE
    dYhat_dZ3 = sigmoid_derivative(Y_hat) // Derivative of sigmoid(Z3) w.r.t Z3
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients
    dZ2 = dA2 * sigmoid_derivative(A2) // A2 = sigmoid(Z2)
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients
    dZ1 = dA1 * sigmoid_derivative(A1) // A1 = sigmoid(Z1)
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: // Print progress periodically
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  Define N
  Generate or load training data X_train (binary inputs), Y_train (binary outputs)
  Set h1_size, h2_size
  Set epochs, learning_rate

  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate)

  Print "Training Complete."
  Print "Final Weights and Biases:"
  Print W1, b1, W2, b2, W3, b3
```

## Python Code

```python
import numpy as np

# Sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

class MLP_Sigmoid:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs

        # Initialize weights and biases
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * 0.1
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * 0.1
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders for activations and weighted sums during backprop
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = sigmoid(self.Z1)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = sigmoid(self.Z2)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3)
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0] # Number of samples

        # Calculate error and loss derivative (using MSE for simplicity)
        error = self.Y_hat - Y
        # Gradient of MSE loss w.r.t Y_hat is 2 * error / m, but the 2/m can be absorbed into learning rate
        # Or simply use 'error' if using Binary Cross Entropy derivative (Y_hat - Y)
        # Let's stick to MSE derivative for consistency with pseudocode
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients
        dZ2 = dA2 * sigmoid_derivative(self.A2)
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients
        dZ1 = dA1 * sigmoid_derivative(self.A1)
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            # Forward pass
            Y_hat = self.forward_pass(X)

            # Calculate loss (MSE)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)

            # Backward pass and update weights
            self.backward_pass(X, Y, learning_rate)

            # Print progress
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}\n{self.W1}")
        print(f"b1 Shape: {self.b1.shape}\n{self.b1}")
        print(f"W2 Shape: {self.W2.shape}\n{self.W2}")
        print(f"b2 Shape: {self.b2.shape}\n{self.b2}")
        print(f"W3 Shape: {self.W3.shape}\n{self.W3}")
        print(f"b3 Shape: {self.b3.shape}\n{self.b3}")

# --- Main Execution ---
if __name__ == "__main__":
    # Example: XOR problem (needs N=2 inputs)
    # For N inputs, generate synthetic data
    N = 4 # Number of inputs (as required by some experiments, adjust if needed)
    num_samples = 100

    # Generate random binary input data
    X_train = np.random.randint(0, 2, size=(num_samples, N))

    # Generate synthetic binary output Y based on some logic (e.g., complex XOR-like)
    # Example: Output is 1 if the sum of inputs is odd, 0 otherwise
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")
    # print("Sample X:", X_train[:5])
    # print("Sample Y:", Y_train[:5])

    # Define network structure
    n_hidden1 = 8 # Number of neurons in first hidden layer
    n_hidden2 = 4 # Number of neurons in second hidden layer
    n_outputs = 1 # Single output neuron

    # Hyperparameters
    epochs = 5000
    learning_rate = 0.1

    # Create and train the MLP
    mlp = MLP_Sigmoid(N, n_hidden1, n_hidden2, n_outputs)
    print("\n--- Training MLP with Sigmoid Activation ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")

    # Display final parameters
    mlp.display_parameters()

    # Optional: Make predictions on training data
    predictions = mlp.predict(X_train)
    # Convert probabilities to binary output (0 or 1) using a threshold
    binary_predictions = (predictions > 0.5).astype(int)

    # Calculate accuracy
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title('Training Loss over Epochs')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.show()

```

**Explanation:**

1.  **`sigmoid` and `sigmoid_derivative`:** Implement the activation function and its derivative.
2.  **`MLP_Sigmoid` Class:**
    *   `__init__`: Initializes the network structure, weights (small random values), and biases (zeros).
    *   `forward_pass`: Computes the output of the network for given input `X`, storing intermediate values (Z1, A1, Z2, A2) needed for backpropagation.
    *   `backward_pass`: Calculates the gradients of the loss with respect to all weights and biases using the chain rule and updates the parameters using gradient descent.
    *   `train`: Iterates through the training data for a specified number of epochs, performing forward and backward passes and recording the loss.
    *   `predict`: Performs a forward pass to get predictions for new data.
    *   `display_parameters`: Shows the final learned weights and biases.
3.  **Main Block:**
    *   Sets `N` (number of inputs).
    *   Generates synthetic binary input data `X_train` and corresponding target output `Y_train`.
    *   Defines the hidden layer sizes, number of epochs, and learning rate.
    *   Creates an instance of the `MLP_Sigmoid` class.
    *   Calls the `train` method.
    *   Calls `display_parameters` to show the final weights and biases.
    *   Calculates and prints the training accuracy and the number of epochs (steps).

This code provides a complete implementation of an MLP with two hidden layers, using the Sigmoid activation function and the backpropagation algorithm for training.