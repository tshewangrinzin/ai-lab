# Experiment 24: MLP with Backpropagation (Swish Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**Swish Activation Function:**

*   **Formula:** `Swish(x) = x * sigmoid(beta * x)`
    *   `beta` is a constant or a trainable parameter. Often, `beta` is set to 1, resulting in `Swish(x) = x * sigmoid(x)`.
*   **Output Range:** Approximately (-0.28, u221e) for `beta=1`.
*   **Derivative (for beta=1):** `Swish_derivative(x) = Swish(x) + sigmoid(x) * (1 - Swish(x))`
    *   Alternatively: `Swish_derivative(x) = sigmoid(x) + x * sigmoid_derivative(sigmoid(x))`
*   **Advantages:**
    *   Non-monotonic: Unlike most common activation functions, Swish can decrease even when the input increases, which might help with optimization.
    *   Smooth and non-linear.
    *   Often performs better than ReLU on deeper models across various tasks.
    *   Unbounded above (like ReLU), avoiding saturation for large positive values.
    *   Bounded below, which can help with regularization.
*   **Disadvantages:**
    *   Computationally more expensive than ReLU due to the sigmoid calculation.
*   **Use:** A promising alternative to ReLU, developed by Google Brain.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **Swish** activation (with `beta=1`).
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.

**Backpropagation with Swish (beta=1):**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the Swish derivative:

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3).
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = Swish(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = Swish(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE)
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (Swish):**
        *   `dZ2 = dA2 * Swish_derivative(Z2, A2)` (Note: Derivative depends on Z2 and A2)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (Swish):**
        *   `dZ1 = dA1 * Swish_derivative(Z1, A1)` (Note: Derivative depends on Z1 and A1)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs
Beta = 1.0 // Swish parameter (fixed)

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function swish(x, beta):
  Return x * sigmoid(beta * x)

Function swish_derivative(z, a, beta): // z is input, a is swish(z)
  sig_beta_z = sigmoid(beta * z)
  Return a + sig_beta_z * (1 - a) // Using the formula: Swish(x) + sigmoid(beta*x)*(1-Swish(x))
  // Alternative: return sig_beta_z + z * beta * sigmoid_derivative(sig_beta_z)

// Initialize Weights and Biases (He init often suitable)
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // ... (He initialization recommended)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate, beta):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using Swish for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = swish(Z1, beta)
    Z2 = A1.dot(W2) + b2
    A2 = swish(Z2, beta)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples
    dYhat_dZ3 = sigmoid_derivative(Y_hat)
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (Swish)
    // Apply element-wise: dA2 * derivative(Z2, A2, beta)
    dZ2 = dA2 * swish_derivative(Z2, A2, beta) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (Swish)
    // Apply element-wise: dA1 * derivative(Z1, A1, beta)
    dZ1 = dA1 * swish_derivative(Z1, A1, beta) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to previous experiments, define N, data, hyperparameters, beta=1)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate, 1.0)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (used by Swish and output layer)
def sigmoid(x):
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# Swish activation function and its derivative (beta=1)
def swish(x, beta=1.0):
    return x * sigmoid(beta * x)

def swish_derivative(z, a, beta=1.0): # z is input, a is swish(z)
    sig_beta_z = sigmoid(beta * z)
    # Using the formula: Swish(x) + sigmoid(beta*x)*(1-Swish(x))
    # Which simplifies to: a + sig_beta_z * (1 - a)
    return a + sig_beta_z * (1 - a)
    # Alternative implementation:
    # return sig_beta_z + z * beta * sigmoid_derivative(sig_beta_z)

class MLP_Swish_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs, beta=1.0):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs
        self.beta = beta # Store beta

        # Initialize weights and biases (He initialization often suitable)
        self.W1 = np.random.randn(self.n_inputs, self.n_hidden1) * np.sqrt(2. / self.n_inputs)
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2) * np.sqrt(2. / self.n_hidden1)
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Output layer (Sigmoid)
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = swish(self.Z1, self.beta)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = swish(self.Z2, self.beta)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0]
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (Swish)
        # Pass Z2 and A2 to the derivative function
        dZ2 = dA2 * swish_derivative(self.Z2, self.A2, self.beta)
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (Swish)
        # Pass Z1 and A1 to the derivative function
        dZ1 = dA1 * swish_derivative(self.Z1, self.A1, self.beta)
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            Y_hat = self.forward_pass(X)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)
            self.backward_pass(X, Y, learning_rate)
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}")
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    N = 4
    num_samples = 100
    X_train = np.random.randint(0, 2, size=(num_samples, N))
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    n_hidden1 = 16
    n_hidden2 = 8
    n_outputs = 1
    swish_beta = 1.0 # Using standard Swish (beta=1)

    epochs = 3000
    learning_rate = 0.05 # May need tuning

    mlp = MLP_Swish_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs, beta=swish_beta)
    print(f"\n--- Training MLP with Swish (beta={swish_beta}) Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")
    mlp.display_parameters()

    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title(f'Training Loss over Epochs (Swish beta={swish_beta})')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`sigmoid` and `sigmoid_derivative`:** Kept as Swish uses sigmoid internally, and it's also used for the output layer.
2.  **`swish` and `swish_derivative`:** Implement the Swish function (`x * sigmoid(beta*x)`) and its derivative. The derivative implementation uses the formula `Swish(x) + sigmoid(beta*x)*(1-Swish(x))`, which requires both the input `z` (to calculate `sigmoid(beta*z)`) and the output `a = swish(z)`.
3.  **`MLP_Swish_SigmoidOutput` Class:**
    *   `__init__`: Initializes the network, storing `beta` (defaulting to 1.0). He initialization is often suitable.
    *   `forward_pass`: Uses `swish` (passing `self.beta`) for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `swish_derivative(self.Z1, self.A1, self.beta)` and `swish_derivative(self.Z2, self.A2, self.beta)` when calculating `dZ1` and `dZ2` respectively, passing both the pre-activation (`Z`) and activation (`A`) values.
    *   `train`, `predict`, `display_parameters`: Similar functionality.
4.  **Main Block:**
    *   Sets up data, network parameters, and defines `swish_beta` (set to 1.0 for standard Swish).
    *   Instantiates the `MLP_Swish_SigmoidOutput` class, passing `beta`.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using Swish (with beta=1) in the hidden layers and Sigmoid in the output layer, trained with backpropagation, demonstrating the implementation of this modern activation function.