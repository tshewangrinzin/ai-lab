# Experiment 17: Simple Multi-Layer Perceptron (N Inputs, 2 Hidden, 1 Output)

## Theory

**Multi-Layer Perceptron (MLP):**

An MLP is a type of feedforward artificial neural network (ANN). It consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer. Except for the input nodes, each node is a neuron that uses a non-linear activation function.

*   **Input Layer:** Receives the initial data (features). In this case, N binary inputs (0 or 1).
*   **Hidden Layers:** Intermediate layers between the input and output layers. They allow the network to learn complex patterns by transforming the input data. This MLP has two hidden layers.
*   **Output Layer:** Produces the final result. Here, it has one neuron producing a single binary output (typically interpreted as 0 or 1, often via a threshold on an activation function like Sigmoid).
*   **Neurons (Nodes):** Each neuron (except input) receives weighted inputs from the previous layer, adds a bias, applies an activation function, and passes the result to the next layer.
*   **Weights:** Parameters associated with the connections between neurons. They determine the strength of the connection. Initialized randomly.
*   **Biases:** Parameters associated with each neuron (except input). They allow shifting the activation function. Initialized randomly.
*   **Activation Function:** Introduces non-linearity, allowing the network to learn complex relationships. Common choices include Sigmoid, ReLU, Tanh. For a binary output, Sigmoid is often used in the output layer.
*   **Feedforward:** Information flows in one direction, from the input layer, through the hidden layers, to the output layer, without cycles.

**Task Specifics:**

This experiment requires creating the structure of an MLP with:

*   `N` binary inputs.
*   A first hidden layer (size can be chosen, e.g., `h1_size`).
*   A second hidden layer (size can be chosen, e.g., `h2_size`).
*   An output layer with 1 neuron for binary output.

The task focuses on initializing the weights and biases randomly and displaying them. It does *not* involve training (like backpropagation) or making predictions, only setting up the initial random state.
The "number of steps" mentioned is ambiguous in this context without training; it might refer to the number of layers (input + hidden + output = 4 steps/layers) or simply be a placeholder. We will display the structure and initial values.

**Weight Matrix Dimensions:**

*   Input to Hidden Layer 1 (W1): `(N, h1_size)`
*   Bias for Hidden Layer 1 (b1): `(1, h1_size)`
*   Hidden Layer 1 to Hidden Layer 2 (W2): `(h1_size, h2_size)`
*   Bias for Hidden Layer 2 (b2): `(1, h2_size)`
*   Hidden Layer 2 to Output Layer (W3): `(h2_size, 1)`
*   Bias for Output Layer (b3): `(1, 1)`

## Pseudocode/Algorithm

```
// Define Network Structure
Input: N (number of binary inputs)
Define h1_size (number of neurons in hidden layer 1)
Define h2_size (number of neurons in hidden layer 2)
Output_size = 1

// Initialize Weights and Biases Randomly
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // Weights
  W1 = Generate random matrix of size (N, h1_size)
  W2 = Generate random matrix of size (h1_size, h2_size)
  W3 = Generate random matrix of size (h2_size, output_size)

  // Biases
  b1 = Generate random vector of size (1, h1_size)
  b2 = Generate random vector of size (1, h2_size)
  b3 = Generate random vector of size (1, output_size)

  Return W1, b1, W2, b2, W3, b3

// Display Function
Function display_parameters(W1, b1, W2, b2, W3, b3):
  Print "--- MLP Structure ---"
  Print "Input Layer Size:", N
  Print "Hidden Layer 1 Size:", h1_size
  Print "Hidden Layer 2 Size:", h2_size
  Print "Output Layer Size:", output_size

  Print "\n--- Initial Random Weights ---"
  Print "W1 (Input -> Hidden1) Matrix Shape:", shape(W1)
  Print W1
  Print "W2 (Hidden1 -> Hidden2) Matrix Shape:", shape(W2)
  Print W2
  Print "W3 (Hidden2 -> Output) Matrix Shape:", shape(W3)
  Print W3

  Print "\n--- Initial Random Biases ---"
  Print "b1 (Hidden1) Vector Shape:", shape(b1)
  Print b1
  Print "b2 (Hidden2) Vector Shape:", shape(b2)
  Print b2
  Print "b3 (Output) Vector Shape:", shape(b3)
  Print b3

  // Interpret "number of steps" as number of layers
  Print "\nNumber of Layers (Steps):", 4 // Input, Hidden1, Hidden2, Output

Main:
  Ask user for N (number of inputs)
  Set h1_size (e.g., 5)
  Set h2_size (e.g., 3)
  output_size = 1

  (W1, b1, W2, b2, W3, b3) = initialize_mlp(N, h1_size, h2_size, output_size)

  display_parameters(W1, b1, W2, b2, W3, b3)
```

## Python Code

```python
import numpy as np

def initialize_mlp(n_inputs, n_hidden1, n_hidden2, n_outputs):
    """Initializes weights and biases for a 2-hidden layer MLP randomly.

    Args:
        n_inputs (int): Number of input features (N).
        n_hidden1 (int): Number of neurons in the first hidden layer.
        n_hidden2 (int): Number of neurons in the second hidden layer.
        n_outputs (int): Number of output neurons (1 for this case).

    Returns:
        tuple: Contains weight matrices (W1, W2, W3) and bias vectors (b1, b2, b3).
    """
    # Initialize weights with small random values (e.g., from standard normal distribution)
    # Using random.randn which gives values from a standard normal distribution.
    # You could also use random.rand for uniform [0, 1) or scale them.
    W1 = np.random.randn(n_inputs, n_hidden1)
    W2 = np.random.randn(n_hidden1, n_hidden2)
    W3 = np.random.randn(n_hidden2, n_outputs)

    # Initialize biases, often starting with zeros or small random values
    b1 = np.random.randn(1, n_hidden1)
    b2 = np.random.randn(1, n_hidden2)
    b3 = np.random.randn(1, n_outputs)

    return W1, b1, W2, b2, W3, b3

def display_parameters(N, h1_size, h2_size, output_size, W1, b1, W2, b2, W3, b3):
    """Displays the network structure and initial parameters."""
    print("--- MLP Structure ---")
    print(f"Input Layer Size (N): {N}")
    print(f"Hidden Layer 1 Size: {h1_size}")
    print(f"Hidden Layer 2 Size: {h2_size}")
    print(f"Output Layer Size: {output_size}")

    print("\n--- Initial Random Weights ---")
    print(f"W1 (Input -> Hidden1) Matrix Shape: {W1.shape}")
    print(W1)
    print(f"\nW2 (Hidden1 -> Hidden2) Matrix Shape: {W2.shape}")
    print(W2)
    print(f"\nW3 (Hidden2 -> Output) Matrix Shape: {W3.shape}")
    print(W3)

    print("\n--- Initial Random Biases ---")
    print(f"b1 (Hidden1) Vector Shape: {b1.shape}")
    print(b1)
    print(f"\nb2 (Hidden2) Vector Shape: {b2.shape}")
    print(b2)
    print(f"\nb3 (Output) Vector Shape: {b3.shape}")
    print(b3)

    # Interpret "number of steps" as the number of layers involved in computation
    # Input layer doesn't compute, so 3 computational layers (2 hidden + 1 output)
    # Or simply the total number of layers = 4
    print(f"\nNumber of Layers (including input): 4")
    print(f"Number of Computational Layers (Steps): 3")

# --- Main Execution ---
if __name__ == "__main__":
    while True:
        try:
            n_inputs = int(input("Enter the number of binary inputs (N): "))
            if n_inputs > 0:
                break
            else:
                print("Number of inputs must be positive.")
        except ValueError:
            print("Invalid input. Please enter an integer.")

    # Define the size of the hidden layers (can be adjusted)
    n_hidden1 = 5
    n_hidden2 = 3
    n_outputs = 1 # Single binary output

    print(f"\nInitializing MLP with N={n_inputs}, Hidden1={n_hidden1}, Hidden2={n_hidden2}, Output={n_outputs}")

    # Initialize the MLP parameters
    W1, b1, W2, b2, W3, b3 = initialize_mlp(n_inputs, n_hidden1, n_hidden2, n_outputs)

    # Display the structure and initial parameters
    display_parameters(n_inputs, n_hidden1, n_hidden2, n_outputs, W1, b1, W2, b2, W3, b3)

```

**Explanation:**

1.  **`initialize_mlp` Function:** Takes the number of neurons in each layer as input. It uses `numpy.random.randn` to create weight matrices and bias vectors with the correct dimensions, filled with random values drawn from a standard normal distribution.
2.  **`display_parameters` Function:** Takes the layer sizes and the initialized weights/biases. It prints the shape and values of each weight matrix (W1, W2, W3) and bias vector (b1, b2, b3). It also clarifies the interpretation of "number of steps" as the number of layers.
3.  **Main Block:**
    *   Prompts the user to enter the number of inputs `N`.
    *   Sets predefined sizes for the two hidden layers (`n_hidden1`, `n_hidden2`). These could also be taken as user input if desired.
    *   Calls `initialize_mlp` to get the random weights and biases.
    *   Calls `display_parameters` to show the results as required by the experiment.

This code fulfills the requirement of setting up the MLP structure with random weights/biases and displaying them, without performing any training or prediction.