# Experiment 21: MLP with Backpropagation (Tanh Activation)

## Theory

**Multi-Layer Perceptron (MLP) & Backpropagation:**

Refer to Experiment 19 for the general concepts of MLPs and the Backpropagation algorithm. This experiment modifies the activation function used in the hidden layers.

**Tanh (Hyperbolic Tangent) Activation Function:**

*   **Formula:** `tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
*   **Output Range:** (-1, 1)
*   **Derivative:** `tanh_derivative(x) = 1 - tanh(x)^2`
*   **Advantages:**
    *   Zero-centered output (unlike Sigmoid), which can help with gradient flow during training.
    *   Stronger gradients than Sigmoid for inputs close to zero, potentially leading to faster convergence.
*   **Disadvantages:**
    *   Still suffers from the vanishing gradient problem for large positive or negative inputs, although generally less severe than Sigmoid.
    *   Computationally slightly more expensive than ReLU.
*   **Use:** Was a popular choice for hidden layers before ReLU gained prominence, still used in certain architectures like LSTMs/GRUs.

**Task Specifics:**

*   N binary inputs.
*   Two hidden layers using **Tanh** activation.
*   One output neuron using **Sigmoid** activation (suitable for binary classification).
*   Implement the backpropagation algorithm for training.

**Backpropagation with Tanh:**

The core backpropagation process remains the same, but the calculation of gradients `dZ` for the hidden layers uses the Tanh derivative:

1.  **Initialization:** Initialize weights (W1, W2, W3) and biases (b1, b2, b3).
2.  **Forward Pass:**
    *   Hidden Layer 1: `Z1 = X.dot(W1) + b1`, `A1 = tanh(Z1)`
    *   Hidden Layer 2: `Z2 = A1.dot(W2) + b2`, `A2 = tanh(Z2)`
    *   Output Layer: `Z3 = A2.dot(W3) + b3`, `Output (Y_hat) = sigmoid(Z3)`
3.  **Calculate Error:** (e.g., MSE)
    *   MSE: `Loss = 1/m * sum((Y - Y_hat)^2)`
4.  **Backward Pass (Gradient Calculation):**
    *   Calculate `dLoss/dY_hat`.
    *   **Output Layer Gradients (Sigmoid):**
        *   `dZ3 = dLoss/dY_hat * sigmoid_derivative(Y_hat)`
        *   `dW3 = A2.T.dot(dZ3)`
        *   `db3 = sum(dZ3, axis=0)`
        *   `dA2 = dZ3.dot(W3.T)`
    *   **Hidden Layer 2 Gradients (Tanh):**
        *   `dZ2 = dA2 * tanh_derivative(A2)` (Note: Derivative depends on A2, the output of tanh)
        *   `dW2 = A1.T.dot(dZ2)`
        *   `db2 = sum(dZ2, axis=0)`
        *   `dA1 = dZ2.dot(W2.T)`
    *   **Hidden Layer 1 Gradients (Tanh):**
        *   `dZ1 = dA1 * tanh_derivative(A1)` (Note: Derivative depends on A1, the output of tanh)
        *   `dW1 = X.T.dot(dZ1)`
        *   `db1 = sum(dZ1, axis=0)`
5.  **Update Weights and Biases:**
    *   `W = W - learning_rate * dW`
    *   `b = b - learning_rate * db`
6.  **Repeat:** Iterate steps 2-5.

## Pseudocode/Algorithm

```
// Define Network Structure (same as Exp 19)
Input: N
Define h1_size, h2_size
Output_size = 1
Learning_rate
Epochs

// Activation Functions
Function sigmoid(x):
  Return 1 / (1 + exp(-x))

Function sigmoid_derivative(x): // x is the output of sigmoid(z)
  Return x * (1 - x)

Function tanh(x):
  Return (exp(x) - exp(-x)) / (exp(x) + exp(-x))

Function tanh_derivative(x): // x is the output of tanh(z)
  Return 1 - x^2

// Initialize Weights and Biases (same as Exp 19)
Function initialize_mlp(N, h1_size, h2_size, output_size):
  // ... (same initialization, maybe Xavier/Glorot for Tanh)
  Return W1, b1, W2, b2, W3, b3

// Training Function
Function train_mlp(X_train, Y_train, N, h1_size, h2_size, output_size, epochs, learning_rate):
  W1, b1, W2, b2, W3, b3 = initialize_mlp(N, h1_size, h2_size, output_size)

  For epoch from 1 to epochs:
    // --- Forward Pass --- (Using Tanh for hidden, Sigmoid for output)
    Z1 = X_train.dot(W1) + b1
    A1 = tanh(Z1)
    Z2 = A1.dot(W2) + b2
    A2 = tanh(Z2)
    Z3 = A2.dot(W3) + b3
    Y_hat = sigmoid(Z3) // Output layer uses sigmoid

    // --- Calculate Error (e.g., MSE) ---
    error = Y_train - Y_hat
    loss = mean(error^2)

    // --- Backward Pass --- 
    // Output Layer Gradients (Sigmoid)
    dLoss_dYhat = -2 * error / number_of_samples // Derivative of MSE
    dYhat_dZ3 = sigmoid_derivative(Y_hat) // Derivative of sigmoid(Z3) w.r.t Z3
    dZ3 = dLoss_dYhat * dYhat_dZ3
    
    dW3 = A2.T.dot(dZ3)
    db3 = sum(dZ3, axis=0, keepdims=True)
    dA2 = dZ3.dot(W3.T)

    // Hidden Layer 2 Gradients (Tanh)
    // Apply element-wise: dA2 * derivative(A2)
    dZ2 = dA2 * tanh_derivative(A2) 
    dW2 = A1.T.dot(dZ2)
    db2 = sum(dZ2, axis=0, keepdims=True)
    dA1 = dZ2.dot(W2.T)

    // Hidden Layer 1 Gradients (Tanh)
    // Apply element-wise: dA1 * derivative(A1)
    dZ1 = dA1 * tanh_derivative(A1) 
    dW1 = X_train.T.dot(dZ1)
    db1 = sum(dZ1, axis=0, keepdims=True)

    // --- Update Weights and Biases ---
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3

    If epoch % 100 == 0: 
      Print "Epoch:", epoch, "Loss:", loss

  Return W1, b1, W2, b2, W3, b3

Main:
  // ... (Similar to Exp 19/20, define N, data, hyperparameters)
  W1, b1, W2, b2, W3, b3 = train_mlp(X_train, Y_train, N, h1_size, h2_size, 1, epochs, learning_rate)
  // ... (Print results)
```

## Python Code

```python
import numpy as np

# Sigmoid activation function (for output layer)
def sigmoid(x):
    # Add clipping to prevent overflow/underflow in exp
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def sigmoid_derivative(x): # x is the output of sigmoid
    return x * (1 - x)

# Tanh activation function and its derivative
def tanh(x):
    # Add clipping to prevent overflow/underflow in exp
    x_clipped = np.clip(x, -500, 500)
    return np.tanh(x_clipped)

def tanh_derivative(x): # x is the output of tanh
    return 1 - x**2

class MLP_Tanh_SigmoidOutput:
    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs):
        self.n_inputs = n_inputs
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_outputs = n_outputs

        # Initialize weights and biases (Xavier/Glorot initialization often preferred for Tanh)
        limit1 = np.sqrt(6. / (self.n_inputs + self.n_hidden1))
        self.W1 = np.random.uniform(-limit1, limit1, (self.n_inputs, self.n_hidden1))
        self.b1 = np.zeros((1, self.n_hidden1))
        
        limit2 = np.sqrt(6. / (self.n_hidden1 + self.n_hidden2))
        self.W2 = np.random.uniform(-limit2, limit2, (self.n_hidden1, self.n_hidden2))
        self.b2 = np.zeros((1, self.n_hidden2))
        
        # Output layer (Sigmoid) might use smaller init or Xavier
        limit3 = np.sqrt(6. / (self.n_hidden2 + self.n_outputs))
        self.W3 = np.random.uniform(-limit3, limit3, (self.n_hidden2, self.n_outputs))
        # self.W3 = np.random.randn(self.n_hidden2, self.n_outputs) * 0.1 # Alternative
        self.b3 = np.zeros((1, self.n_outputs))

        # Placeholders for activations and weighted sums during backprop
        self.Z1, self.A1 = None, None
        self.Z2, self.A2 = None, None
        self.Z3, self.Y_hat = None, None

    def forward_pass(self, X):
        self.Z1 = X.dot(self.W1) + self.b1
        self.A1 = tanh(self.Z1)
        self.Z2 = self.A1.dot(self.W2) + self.b2
        self.A2 = tanh(self.Z2)
        self.Z3 = self.A2.dot(self.W3) + self.b3
        self.Y_hat = sigmoid(self.Z3) # Sigmoid output
        return self.Y_hat

    def backward_pass(self, X, Y, learning_rate):
        m = Y.shape[0] # Number of samples

        # Calculate error and loss derivative (using MSE for simplicity)
        error = self.Y_hat - Y
        dLoss_dYhat = 2 * error / m

        # Output Layer Gradients (Sigmoid)
        dYhat_dZ3 = sigmoid_derivative(self.Y_hat)
        dZ3 = dLoss_dYhat * dYhat_dZ3
        dW3 = self.A2.T.dot(dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        dA2 = dZ3.dot(self.W3.T)

        # Hidden Layer 2 Gradients (Tanh)
        dZ2 = dA2 * tanh_derivative(self.A2) # Use A2 for Tanh derivative
        dW2 = self.A1.T.dot(dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        dA1 = dZ2.dot(self.W2.T)

        # Hidden Layer 1 Gradients (Tanh)
        dZ1 = dA1 * tanh_derivative(self.A1) # Use A1 for Tanh derivative
        dW1 = X.T.dot(dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)

        # Update Weights and Biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3

    def train(self, X, Y, epochs, learning_rate):
        history = []
        for epoch in range(epochs):
            # Forward pass
            Y_hat = self.forward_pass(X)

            # Calculate loss (MSE)
            loss = np.mean((Y_hat - Y)**2)
            history.append(loss)

            # Backward pass and update weights
            self.backward_pass(X, Y, learning_rate)

            # Print progress
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        return history

    def predict(self, X):
        return self.forward_pass(X)

    def display_parameters(self):
        print("--- Final Weights and Biases ---")
        print(f"W1 Shape: {self.W1.shape}") # Optionally print weights
        print(f"b1 Shape: {self.b1.shape}")
        print(f"W2 Shape: {self.W2.shape}")
        print(f"b2 Shape: {self.b2.shape}")
        print(f"W3 Shape: {self.W3.shape}")
        print(f"b3 Shape: {self.b3.shape}")

# --- Main Execution ---
if __name__ == "__main__":
    # Example: XOR-like problem for N inputs
    N = 4 # Number of inputs
    num_samples = 100

    # Generate random binary input data
    X_train = np.random.randint(0, 2, size=(num_samples, N))

    # Generate synthetic binary output Y (e.g., Output is 1 if sum of inputs is odd)
    Y_train = (np.sum(X_train, axis=1) % 2).reshape(-1, 1)

    print(f"Generated {num_samples} samples with N={N} inputs.")

    # Define network structure
    n_hidden1 = 10 # Tanh might work well with fewer neurons than ReLU sometimes
    n_hidden2 = 5
    n_outputs = 1 # Single output neuron (Sigmoid)

    # Hyperparameters (May need tuning for Tanh)
    epochs = 2000
    learning_rate = 0.1 # Tanh can sometimes handle slightly larger learning rates than Sigmoid

    # Create and train the MLP
    mlp = MLP_Tanh_SigmoidOutput(N, n_hidden1, n_hidden2, n_outputs)
    print("\n--- Training MLP with Tanh Hidden Layers & Sigmoid Output ---")
    loss_history = mlp.train(X_train, Y_train, epochs, learning_rate)

    print("\nTraining complete.")

    # Display final parameters
    mlp.display_parameters()

    # Make predictions
    predictions = mlp.predict(X_train)
    binary_predictions = (predictions > 0.5).astype(int)

    # Calculate accuracy
    accuracy = np.mean(binary_predictions == Y_train) * 100
    print(f"\nFinal Training Accuracy: {accuracy:.2f}%")
    print(f"Number of steps (epochs): {epochs}")

    # Optional: Plot loss history
    # import matplotlib.pyplot as plt
    # plt.plot(loss_history)
    # plt.title('Training Loss over Epochs (Tanh)')
    # plt.xlabel('Epoch')
    # plt.ylabel('Mean Squared Error Loss')
    # plt.grid(True)
    # plt.show()

```

**Explanation:**

1.  **`tanh` and `tanh_derivative`:** Implement the Tanh function and its derivative. Note that the derivative takes the *output* of Tanh (`A`) as its argument, unlike ReLU's derivative which takes the input (`Z`). Clipping is added to `tanh` to prevent potential numerical issues with `exp`.
2.  **`sigmoid` and `sigmoid_derivative`:** Kept for the output layer.
3.  **`MLP_Tanh_SigmoidOutput` Class:**
    *   `__init__`: Initializes the network. Xavier/Glorot initialization (`np.sqrt(6. / (n_in + n_out))`) is used, which is often recommended for Tanh layers.
    *   `forward_pass`: Uses `tanh` for hidden layers (`A1`, `A2`) and `sigmoid` for the output layer (`Y_hat`).
    *   `backward_pass`: Calculates gradients. The key change is using `tanh_derivative(self.A1)` and `tanh_derivative(self.A2)` when calculating `dZ1` and `dZ2` respectively. The output layer gradient calculation remains the same.
    *   `train`, `predict`, `display_parameters`: Similar functionality to previous versions.
4.  **Main Block:**
    *   Sets up the data and network parameters.
    *   Hyperparameters like hidden layer sizes, `epochs`, and `learning_rate` might need different values compared to Sigmoid or ReLU networks.
    *   Trains the network and evaluates accuracy.

This code implements an MLP using Tanh in the hidden layers and Sigmoid in the output layer, trained with backpropagation.